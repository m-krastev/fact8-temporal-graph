#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --job-name=FACT8_explain
#SBATCH --output=%A.out
#SBATCH --time=12:59:00
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=4
# 

# Load modules
module purge
module load 2023
module load Python/3.11.3-GCCcore-12.3.0

# Activate your environment

# Print current datetime for debug purposes
date

# Correct some running errors
export ROOT="/home/scur1382/fact8" # change with your root folder
export PYTHONPATH=$ROOT:$PYTHONPATH:.

source $ROOT/.venv/bin/activate

# simulate_v1, simulate_v2, wikipedia, reddit
sim_datasets=(simulate_v1 simulate_v2)
real_datasets=(wikipedia reddit)


# Data downloading and preprocessing
# curl http://snap.stanford.edu/jodie/reddit.csv > $ROOT/tgnnexplainer/xgraph/dataset/data/reddit.csv
# curl http://snap.stanford.edu/jodie/wikipedia.csv > $ROOT/tgnnexplainer/xgraph/dataset/data/wikipedia.csv

# # process the real dataset
# cd  $ROOT/tgnnexplainer/xgraph/models/ext/tgat
# python process.py -d wikipedia
# python process.py -d reddit

# # generate simulated dataset
# cd  $ROOT/tgnnexplainer/xgraph/dataset
# python generate_simulate_dataset.py -d simulate_v1
# python generate_simulate_dataset.py -d simulate_v2

# cd $ROOT/tgnnexplainer/xgraph/dataset
# python tg_dataset.py -d wikipedia -c index
# python tg_dataset.py -d reddit -c index
# python tg_dataset.py -d simulate_v1 -c index
# python tg_dataset.py -d simulate_v2 -c index



################################ TGAT ################################
model=tgat

cd  $ROOT/tgnnexplainer/xgraph/models/ext/tgat

sim_datasets=(simulate_v2)
# real_datasets=(wikipedia)
runs=(0)

for run in ${runs[@]}
do
    # echo "Iteration no. ${run}\n"

    for dataset in ${sim_datasets[@]}
    do
        echo "dataset: ${dataset}\n"
        python learn_simulate.py -d ${dataset} --bs 256 --n_degree 10 --n_epoch 90 --agg_method attn --attn_mode prod --gpu 1 --n_head 2 --prefix ${dataset}

        epoch=89
        source_path=./saved_checkpoints/${dataset}-attn-prod-${epoch}.pth
        target_path=$ROOT/tgnnexplainer/xgraph/models/checkpoints/${model}_${dataset}_best.pth
        cp ${source_path} ${target_path}

        echo ${source_path} ${target_path} 'copied'
    done

    # for dataset in ${real_datasets[@]}
    # do
    #     echo "dataset: ${dataset}\n"
    #     python learn_edge.py -d ${dataset} --bs 512 --n_degree 10 --n_epoch 10 --agg_method attn --attn_mode prod --gpu 1 --n_head 2 --prefix ${dataset}

    #         epoch=9
    #         source_path=./saved_checkpoints/${dataset}-attn-prod-${epoch}.pth


    #         mkdir -p $ROOT/tgnnexplainer/xgraph/models/checkpoints
    #         target_path=$ROOT/tgnnexplainer/xgraph/models/checkpoints/${model}_${dataset}_best.pth
    #         cp ${source_path} ${target_path}

    #         echo ${source_path} ${target_path} 'copied'

    # done
done


# ls -l $ROOT/tgnnexplainer/xgraph/models/checkpoints

################################ TGAT ################################




################################ TGN  ################################

cd  $ROOT/tgnnexplainer/xgraph/models/ext/tgn
model=tgn

runs=(0)
for run in ${runs[@]}
do
    # echo "Iteration no. ${run}\n"

    # for dataset in ${sim_datasets[@]}
    # do
    #     echo "dataset: ${dataset}\n"
    #     python train_simulate.py -d ${dataset} --prefix tgn-attn --n_runs 1 --n_epoch 100 --n_layer 2 --n_degree 10 --use_memory --memory_update_at_end --gpu 0 \
    #     --memory_dim 4 # memory_dim should equal to node/edge feature dim


    #     epoch=99

    #     mkdir -p $ROOT/tgnnexplainer/xgraph/models/checkpoints
    #     source_path=./saved_checkpoints/tgn-attn-${dataset}-${epoch}.pth
    #     target_path=$ROOT/tgnnexplainer/models/checkpoints/${model}_${dataset}_best.pth
    #     cp ${source_path} ${target_path}
    #     echo ${source_path} ${target_path} 'copied'

    #     ls -l /xgraph/models/checkpoints | grep '.*best.pth'

    # done

    # real_datasets=(reddit)
    # for dataset in ${real_datasets[@]}
    # do
    #     echo "dataset: ${dataset}\n"
    #     python train_self_supervised.py -d ${dataset} --prefix tgn-attn --n_runs 1 --n_epoch 20 --n_layer 2 --n_degree 10 --use_memory --gpu 0

    #     # Save last epoch
    #     epoch=19
    #     mkdir -p $ROOT/tgnnexplainer/xgraph/models/checkpoints
    #     source_path=./saved_checkpoints/tgn-attn-${dataset}-${epoch}.pth
    #     target_path=$ROOT/tgnnexplainer/xgraph/models/checkpoints/${model}_${dataset}_best.pth
    #     cp ${source_path} ${target_path}
    #     echo ${source_path} ${target_path} 'copied'

    #     ls -l $ROOT/tgnnexplainer/xgraph/models/checkpoints | grep '.*best.pth'
    # done

done

################################ TGN  ################################




################################ RUN EVALUATION  ################################
# cd  $ROOT/benchmarks/xgraph

# models=("tgat tgn")
# datasets=("wikipedia")
# for dataset in ${datasets}
# do
#     echo "dataset: ${dataset}\n"
#     for model in ${models}
#     do
#         echo "model: ${model}\n"
#         # ours
#         python subgraphx_tg_run.py datasets=${dataset} device_id=0 explainers=subgraphx_tg models=${model}

#         # baselines
#         python subgraphx_tg_run.py datasets=${dataset} device_id=0 explainers=attn_explainer_tg models=${model}
#         python subgraphx_tg_run.py datasets=${dataset} device_id=0 explainers=pg_explainer_tg models=${model}
#         python subgraphx_tg_run.py datasets=${dataset} device_id=0 explainers=pbone_explainer_tg models=${model}
#     done
# done